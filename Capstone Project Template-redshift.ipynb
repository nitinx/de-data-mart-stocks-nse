{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "India's National Stock Exchange (NSE) historical data spanning close to 20 years has been sourced in CSV format. Each stock has a file of its own and there are in excess of 1300 stocks. Additionally, a static file has been provided which maps the stock symbol to the Company. This data is to be processed and populated into Amazon Redshift for analytics. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import os\n",
    "import datetime\n",
    "import configparser\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Source data is in CSV format and is in the form of multiple files - each stock file has records for a single stock from 2000 onwards. There are 1386 files in all. Additionally, there is one static file mapping the stock symbol to the company. \n",
    "1. Transform grouping of source data from symbol to date based (to support sourcing additional data from the future)\n",
    "2. Transform data from Step #1 and populate a data late (hosted on S3)\n",
    "3. Transform data from Step #1 and populate a data warehouse (hosted on Redshift)\n",
    "4. Run data quality checks to validate the load. \n",
    "5. Data would be leveraged for data analytics. \n",
    "\n",
    "Tool Usage:\n",
    "1. Apache Spark (PySpark) for data exploration\n",
    "2. AWS S3 for staging of source/intermediate files\n",
    "3. AWS Redshift for hosting warehouse\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Dataset contains two categories of data:\n",
    "1. Stock Data: 1386 files in CSV format which contains stock prices for 1364 NSE stocks from 2000 onwards. One record/stock for each trading day. In all, there are 3,758,123 records. Columns include Date, Open, Close, High, Low and Volume. \n",
    "\n",
    "2. Companies Data: 1 file in CSV format which maps the Stock Symbol to the Company. Contains 1384 records. \n",
    "\n",
    "Dataset has been obtained from Kaggle (https://www.kaggle.com/abhishekyana/nse-listed-1384-companies-data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "stock_data = \"s3a://nse-stock-data/stocks/*/*.csv\"\n",
    "companies_data = \"s3a://nse-stock-data/static/Companies_list.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_companies = spark.read.csv(companies_data, header=True, inferSchema=True)\n",
    "df_stocks = spark.read.csv(stock_data, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1384, 3756739)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_companies.count(), df_stocks.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- Symbol: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_companies.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20MICRONS</td>\n",
       "      <td>20_Microns_Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3IINFOTECH</td>\n",
       "      <td>3i_Infotech_Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3MINDIA</td>\n",
       "      <td>3M_India_Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A2ZMES</td>\n",
       "      <td>A2Z_Maintenance_&amp;_Engineering_Services_Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AANJANEYA</td>\n",
       "      <td>Aanjaneya_Lifecare_Limited</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _c0      Symbol                                            Name\n",
       "0    0   20MICRONS                              20_Microns_Limited\n",
       "1    1  3IINFOTECH                             3i_Infotech_Limited\n",
       "2    2     3MINDIA                                3M_India_Limited\n",
       "3    3      A2ZMES  A2Z_Maintenance_&_Engineering_Services_Limited\n",
       "4    4   AANJANEYA                      Aanjaneya_Lifecare_Limited"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 300)\n",
    "df_companies.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- open: string (nullable = true)\n",
      " |-- high: string (nullable = true)\n",
      " |-- low: string (nullable = true)\n",
      " |-- close: string (nullable = true)\n",
      " |-- adj_close: string (nullable = true)\n",
      " |-- volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stocks.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20MICRONS</td>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>33.349998</td>\n",
       "      <td>31.799999</td>\n",
       "      <td>31.950001</td>\n",
       "      <td>31.269989</td>\n",
       "      <td>29363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20MICRONS</td>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>32.700001</td>\n",
       "      <td>32.750000</td>\n",
       "      <td>31.850000</td>\n",
       "      <td>31.900000</td>\n",
       "      <td>31.221052</td>\n",
       "      <td>35751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20MICRONS</td>\n",
       "      <td>2015-07-03</td>\n",
       "      <td>32.349998</td>\n",
       "      <td>32.400002</td>\n",
       "      <td>31.750000</td>\n",
       "      <td>32.150002</td>\n",
       "      <td>31.465734</td>\n",
       "      <td>13035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20MICRONS</td>\n",
       "      <td>2015-07-06</td>\n",
       "      <td>31.650000</td>\n",
       "      <td>32.900002</td>\n",
       "      <td>31.650000</td>\n",
       "      <td>31.850000</td>\n",
       "      <td>31.172117</td>\n",
       "      <td>25504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20MICRONS</td>\n",
       "      <td>2015-07-07</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.900002</td>\n",
       "      <td>31.750000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>31.318926</td>\n",
       "      <td>45904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      symbol       Date       open       high        low      close  \\\n",
       "0  20MICRONS 2015-07-01  33.000000  33.349998  31.799999  31.950001   \n",
       "1  20MICRONS 2015-07-02  32.700001  32.750000  31.850000  31.900000   \n",
       "2  20MICRONS 2015-07-03  32.349998  32.400002  31.750000  32.150002   \n",
       "3  20MICRONS 2015-07-06  31.650000  32.900002  31.650000  31.850000   \n",
       "4  20MICRONS 2015-07-07  32.000000  32.900002  31.750000  32.000000   \n",
       "\n",
       "   adj_close volume  \n",
       "0  31.269989  29363  \n",
       "1  31.221052  35751  \n",
       "2  31.465734  13035  \n",
       "3  31.172117  25504  \n",
       "4  31.318926  45904  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 300)\n",
    "df_stocks.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|summary|   symbol|              open|              high|               low|             close|         adj_close|            volume|\n",
      "+-------+---------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|  count|  3756739|           3756739|           3756739|           3756739|           3756739|           3756739|           3756739|\n",
      "|   mean|     null|259.18565048165317| 263.8717590721991|254.43260375896116|258.72030193729694|243.76858077967336|1042479.7592680976|\n",
      "| stddev|     null|1351.0480473503249|1367.8412675832865|1332.6522692561446|1349.1390782857247|1338.9817958504302| 8203915.916546446|\n",
      "|    min|20MICRONS|          0.000000|              0.05|              0.05|              0.05|         -0.000028|                 0|\n",
      "|    25%|     null|         24.049999|             24.77|         23.450001|              24.0|         19.419813|            6928.0|\n",
      "|    50%|     null|              67.5|            69.125|         65.962502|         67.300003|         57.900002|           47120.0|\n",
      "|    75%|     null|        186.399994|        190.514008|           182.444|             186.0|        166.956589|          332322.0|\n",
      "|    max|    ZYLOG|              null|              null|              null|              null|              null|              null|\n",
      "+-------+---------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stocks.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### From the summary above:\n",
    "1. It seems like the numeric columns open, high, low, close, adj_close and volume have non-numeric data. \n",
    "2. adj_close has negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+----+----+----+-----+---------+------+\n",
      "|    symbol|               Date|open|high| low|close|adj_close|volume|\n",
      "+----------+-------------------+----+----+----+-----+---------+------+\n",
      "|JAYSREETEA|2015-07-01 00:00:00|null|null|null| null|     null|  null|\n",
      "|JAYSREETEA|2015-07-06 00:00:00|null|null|null| null|     null|  null|\n",
      "|JAYSREETEA|2015-07-07 00:00:00|null|null|null| null|     null|  null|\n",
      "|JAYSREETEA|2015-07-08 00:00:00|null|null|null| null|     null|  null|\n",
      "|JAYSREETEA|2015-07-09 00:00:00|null|null|null| null|     null|  null|\n",
      "|JAYSREETEA|2015-07-14 00:00:00|null|null|null| null|     null|  null|\n",
      "|JAYSREETEA|2015-07-15 00:00:00|null|null|null| null|     null|  null|\n",
      "|JAYSREETEA|2015-07-16 00:00:00|null|null|null| null|     null|  null|\n",
      "|JAYSREETEA|2015-07-17 00:00:00|null|null|null| null|     null|  null|\n",
      "|JAYSREETEA|2015-07-22 00:00:00|null|null|null| null|     null|  null|\n",
      "|JAYSREETEA|2015-07-23 00:00:00|null|null|null| null|     null|  null|\n",
      "|JAYSREETEA|2015-07-27 00:00:00|null|null|null| null|     null|  null|\n",
      "|JAYSREETEA|2015-07-28 00:00:00|null|null|null| null|     null|  null|\n",
      "|JAYSREETEA|2015-07-29 00:00:00|null|null|null| null|     null|  null|\n",
      "|JAYSREETEA|2015-07-30 00:00:00|null|null|null| null|     null|  null|\n",
      "|JAYSREETEA|2015-07-31 00:00:00|null|null|null| null|     null|  null|\n",
      "|LAKSHMIEFL|2015-07-01 00:00:00|null|null|null| null|     null|  null|\n",
      "|LAKSHMIEFL|2015-07-02 00:00:00|null|null|null| null|     null|  null|\n",
      "|LAKSHMIEFL|2015-07-03 00:00:00|null|null|null| null|     null|  null|\n",
      "|LAKSHMIEFL|2015-07-06 00:00:00|null|null|null| null|     null|  null|\n",
      "+----------+-------------------+----+----+----+-----+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stocks.filter(df_stocks['open'] == 'null').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3689727"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stocks.filter(df_stocks['open'] != 'null').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### As per the count obtained above, there are 67012 records with numeric columns having value 'null'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "schema_stocks = StructType([\n",
    "    StructField(\"symbol\", StringType()),\n",
    "    StructField(\"date\", DateType()),\n",
    "    StructField(\"open\", DoubleType()),\n",
    "    StructField(\"high\", DoubleType()),\n",
    "    StructField(\"low\", DoubleType()),\n",
    "    StructField(\"close\", DoubleType()),\n",
    "    StructField(\"adj_close\", DoubleType()),\n",
    "    StructField(\"volume\", IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_stocks = spark.read.csv(stock_data, header=True, schema=schema_stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+-----+-----+-----+---------+------+\n",
      "|    symbol|      date| open| high|  low|close|adj_close|volume|\n",
      "+----------+----------+-----+-----+-----+-----+---------+------+\n",
      "|BANCOINDIA|2007-03-01| 2.89|2.955| 2.74|2.939|-1.694174| 14360|\n",
      "|BANCOINDIA|2007-03-02|2.851|2.851|2.773|2.773|-1.598485|  5460|\n",
      "|BANCOINDIA|2007-03-05| 2.74|2.906| 2.67| 2.67| -1.53911|  5150|\n",
      "|BANCOINDIA|2007-03-06| 2.69|2.715| 2.64|2.715| -1.56505|  3160|\n",
      "|BANCOINDIA|2007-03-07| 2.65| 2.65|2.628|2.637|-1.520088|  1650|\n",
      "|BANCOINDIA|2007-03-08|2.901|2.901| 2.64| 2.64|-1.521817|    20|\n",
      "|BANCOINDIA|2007-03-09| 2.75| 2.75|2.646|2.737|-1.577732|  4820|\n",
      "|BANCOINDIA|2007-03-12| 2.76| 2.76|2.623|2.648|-1.526429|  9230|\n",
      "|BANCOINDIA|2007-03-13| 2.68| 2.73| 2.66|2.705|-1.559286|  5680|\n",
      "|BANCOINDIA|2007-03-14| 2.79| 2.79| 2.72| 2.72|-1.567933|  1390|\n",
      "|BANCOINDIA|2007-03-15| 2.72|2.749| 2.71|2.735| -1.57658|  2570|\n",
      "|BANCOINDIA|2007-03-16|  2.7| 2.75| 2.69| 2.69| -1.55064|  3120|\n",
      "|BANCOINDIA|2007-03-19| 2.68| 2.68|2.651|2.651|-1.528158|  7250|\n",
      "|BANCOINDIA|2007-03-20| 2.66| 2.66|2.566| 2.64|-1.521817|  6250|\n",
      "|BANCOINDIA|2007-03-21| 2.66| 2.74| 2.66| 2.68|-1.544875|  7500|\n",
      "|BANCOINDIA|2007-01-02| 2.84| 2.86| 2.84| 2.86|-1.648635|  1030|\n",
      "|BANCOINDIA|2007-01-03|2.851| 2.86|2.851| 2.86|-1.648635|   220|\n",
      "|BANCOINDIA|2007-01-04|2.806|2.841|2.806|2.841|-1.637683|  2200|\n",
      "|BANCOINDIA|2007-01-05| 2.92| 2.92| 2.92| 2.92|-1.683222|  1000|\n",
      "|BANCOINDIA|2007-01-08| 2.89| 2.94| 2.89| 2.94|-1.694751|   600|\n",
      "+----------+----------+-----+-----+-----+-----+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stocks.filter(df_stocks['adj_close'] < 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8955"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stocks.filter(df_stocks['adj_close'] < 0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### As per the count obtained above, there are 8955 records with negative valued adj_close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "df_stocks_cleaned = df_stocks.withColumn('adj_close', when(df_stocks['adj_close'] < 0, df_stocks['close']).otherwise(df_stocks['adj_close']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "df_stocks_cleaned.filter(df_stocks_cleaned['adj_close'] < 0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Source data would be mapped into 2 dimensions and 1 fact table:\n",
    "\n",
    "1. Stocks \n",
    "    - Table Type: Fact\n",
    "    - Would host transactional data (stock prices) and have columns such as Date, Symbol, Stock Price (Open, Close, High, Low, Adjusted Close) and Volume. \n",
    "    - Table would be partitioned on Symbol (as most analytic queries would be limited to a particular stock (and not across stocks) to identify trends across a time period)\n",
    "    \n",
    "2. Companies\n",
    "    - Table Type: Dimension\n",
    "    - Would host static data (company names to symbol mapping) and have columns such as Stock Symbol and Company Name. \n",
    "    \n",
    "3. Date\n",
    "    - Table Type: Dimension\n",
    "    - Would host static data (date information) and have columns such as Date, Year, Quarter, Month, Week & Day. \n",
    "\n",
    "This model has been arrived at to facilitate analysis of stock prices for a particular stock/company across a time period (Year, Quarter, Month, Week etc).\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "- Populate Data Warehouse (AWS Redshift)\n",
    "    - Generate data for date dimension as a CSV file and load into the the dimension table\n",
    "    - Transform source data (static company details) and populate the compabies dimension table.\n",
    "    - Transform source data (stock prices) and populate the stage stocks table\n",
    "    - Transform the stage stocks data and populate the stocks fact table partitioned by symbol. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### STEP #1: Generate date dimension data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "static_data = \"s3a://nse-stock-data/static/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_date = pd.DataFrame({\"Date\": pd.date_range(start='2000-01-01', end='2020-12-31')})\n",
    "df_date[\"Year\"] = df_date.Date.dt.year\n",
    "df_date[\"Quarter\"] = df_date.Date.dt.quarter\n",
    "df_date[\"Month\"] = df_date.Date.dt.month\n",
    "df_date[\"Week\"] = df_date.Date.dt.weekofyear\n",
    "df_date[\"Day\"] = df_date.Date.dt.weekday_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_date[\"Date\"] = df_date.Date.dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "date = spark.createDataFrame(df_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "date.write.csv(static_data + \"date.csv\", mode='overwrite', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### STEP #2: Connect to AWS Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "KEY=config.get('AWS','AWS_ACCESS_KEY_ID')\n",
    "SECRET= config.get('AWS','AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "DWH_DB= config.get(\"CLUSTER\",\"DB_NAME\")\n",
    "DWH_DB_USER= config.get(\"CLUSTER\",\"DB_USER\")\n",
    "DWH_DB_PASSWORD= config.get(\"CLUSTER\",\"DB_PASSWORD\")\n",
    "DWH_PORT = config.get(\"CLUSTER\",\"DB_PORT\")\n",
    "\n",
    "DWH_ENDPOINT=config.get(\"CLUSTER\",\"HOST\")    \n",
    "DWH_ROLE_ARN=config.get(\"IAM_ROLE\",\"ARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://dwhuser:Passw0rd@dwhcluster.ckj67osxtkpg.us-east-1.redshift.amazonaws.com:5439/dev\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Connected: dwhuser@dev'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### STEP #3: Create the schema and tables in Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.ckj67osxtkpg.us-east-1.redshift.amazonaws.com:5439/dev\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "(psycopg2.ProgrammingError) Relation \"dim_date\" already exists\n",
      " [SQL: 'CREATE TABLE dim_date\\n(\\n  \"date\"       date NOT NULL sortkey,\\n  year         smallint NOT NULL,\\n  quarter      smallint NOT NULL,\\n  month        smallint NOT NULL,\\n  week         smallint NOT NULL,\\n  day          varchar(20) NOT NULL\\n);']\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS stocks;\n",
    "SET search_path TO stocks;\n",
    "\n",
    "DROP TABLE IF EXISTS stage_stocks;\n",
    "DROP TABLE IF EXISTS fact_stocks;\n",
    "DROP TABLE IF EXISTS dim_companies;\n",
    "DROP TABLE IF EXISTS dim_time;\n",
    "\n",
    "CREATE TABLE stage_stocks\n",
    "(\n",
    "  symbol          varchar(100),\n",
    "  Date            varchar(20),\n",
    "  \"open\"          varchar(20),\n",
    "  \"high\"          varchar(20),\n",
    "  \"low\"           varchar(20),\n",
    "  \"close\"         varchar(20),\n",
    "  adj_close       varchar(20),\n",
    "  volume          varchar(20)\n",
    ");\n",
    "\n",
    "CREATE TABLE fact_stocks\n",
    "(\n",
    "  id              bigint IDENTITY(0,1) PRIMARY KEY,\n",
    "  symbol          varchar(100) NOT NULL distkey,\n",
    "  \"date\"          date NOT NULL sortkey,\n",
    "  \"open\"          numeric(20,6),\n",
    "  \"high\"          numeric(20,6),\n",
    "  \"low\"           numeric(20,6),\n",
    "  \"close\"         numeric(20,6),\n",
    "  adjusted_close  numeric(20,6),\n",
    "  volume          bigint\n",
    ");\n",
    "\n",
    "CREATE TABLE dim_companies\n",
    "(\n",
    "  id                 smallint NOT NULL,\n",
    "  symbol             varchar(100),\n",
    "  company_name       varchar(500)\n",
    ");\n",
    "\n",
    "CREATE TABLE dim_date\n",
    "(\n",
    "  \"date\"       date NOT NULL sortkey,\n",
    "  year         smallint NOT NULL,\n",
    "  quarter      smallint NOT NULL,\n",
    "  month        smallint NOT NULL,\n",
    "  week         smallint NOT NULL,\n",
    "  day          varchar(20) NOT NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### STEP #4: Populate the dimensions (date and companies) and stage table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.ckj67osxtkpg.us-east-1.redshift.amazonaws.com:5439/dev\n",
      "Done.\n",
      "CPU times: user 4.33 ms, sys: 510 Âµs, total: 4.84 ms\n",
      "Wall time: 695 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "    copy dim_date from 's3://nse-stock-data/static/date.csv' \n",
    "    iam_role '{}' \n",
    "    ignoreheader as 1\n",
    "    delimiter ','\n",
    "    region 'us-east-1';\n",
    "\"\"\".format(DWH_ROLE_ARN)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.ckj67osxtkpg.us-east-1.redshift.amazonaws.com:5439/dev\n",
      "Done.\n",
      "CPU times: user 4.21 ms, sys: 0 ns, total: 4.21 ms\n",
      "Wall time: 557 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "    copy dim_companies from 's3://nse-stock-data/static/Companies_list.csv' \n",
    "    iam_role '{}' \n",
    "    ignoreheader as 1\n",
    "    delimiter ','\n",
    "    region 'us-east-1';\n",
    "\"\"\".format(DWH_ROLE_ARN)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.ckj67osxtkpg.us-east-1.redshift.amazonaws.com:5439/dev\n",
      "Done.\n",
      "CPU times: user 5.96 ms, sys: 0 ns, total: 5.96 ms\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "    copy stage_stocks from 's3://nse-stock-data/stocks' \n",
    "    iam_role '{}' \n",
    "    ignoreheader as 1\n",
    "    delimiter ','\n",
    "    emptyasnull\n",
    "    blanksasnull    \n",
    "    region 'us-east-1';\n",
    "\"\"\".format(DWH_ROLE_ARN)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### STEP #5: Populate the fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.ckj67osxtkpg.us-east-1.redshift.amazonaws.com:5439/dev\n",
      "3689727 rows affected.\n",
      "CPU times: user 5.91 ms, sys: 3.3 ms, total: 9.21 ms\n",
      "Wall time: 5.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "INSERT INTO fact_stocks (symbol, \"date\", \"open\", \"high\", \"low\", \"close\", adjusted_close, volume)\n",
    "SELECT symbol AS symbol,\n",
    "       CAST(\"date\" as date) AS \"date\",\n",
    "       CAST(\"open\" as numeric(20,6)) AS \"open\",\n",
    "       CAST(\"high\" as numeric(20,6)) AS \"high\",\n",
    "       CAST(\"low\" as numeric(20,6)) AS \"low\",\n",
    "       CAST(\"close\" as numeric(20,6)) AS \"close\",\n",
    "       CAST(adj_close as numeric(20,6)) AS adjusted_close,\n",
    "       CAST(volume as bigint) AS volume\n",
    "  FROM stocks.stage_stocks\n",
    " WHERE \"open\" != 'null';\n",
    "\"\"\"\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### CHECK #1: Compare Source Vs Target records counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.ckj67osxtkpg.us-east-1.redshift.amazonaws.com:5439/dev\n",
      "1 rows affected.\n",
      " * postgresql://dwhuser:***@dwhcluster.ckj67osxtkpg.us-east-1.redshift.amazonaws.com:5439/dev\n",
      "1 rows affected.\n",
      " * postgresql://dwhuser:***@dwhcluster.ckj67osxtkpg.us-east-1.redshift.amazonaws.com:5439/dev\n",
      "1 rows affected.\n"
     ]
    }
   ],
   "source": [
    "cnt_dim_companies = %sql select count(*) from stocks.dim_companies;  \n",
    "cnt_dim_date = %sql select count(*) from stocks.dim_date;  \n",
    "cnt_stage_stocks = %sql select count(*) from stocks.stage_stocks;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension Load - Companies: PASS\n",
      "Dimension Load - Date: FAIL\n",
      "Stage Load - Stocks: PASS\n"
     ]
    }
   ],
   "source": [
    "# Dimension Companies\n",
    "if df_companies.count() == cnt_dim_companies[0][0]:\n",
    "    print('Dimension Load - Companies: PASS')\n",
    "else:\n",
    "    print('Dimension Load - Companies: FAIL')\n",
    "\n",
    "# Dimension Date\n",
    "if date.count() == cnt_dim_date[0][0]:\n",
    "    print('Dimension Load - Date: PASS')\n",
    "else:\n",
    "    print('Dimension Load - Date: FAIL')\n",
    "\n",
    "# Stage Stocks\n",
    "if df_stocks.count() == cnt_stage_stocks[0][0]:\n",
    "    print('Stage Load - Stocks: PASS')\n",
    "else:\n",
    "    print('Stage Load - Stocks: FAIL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.ckj67osxtkpg.us-east-1.redshift.amazonaws.com:5439/dev\n",
      "1 rows affected.\n",
      " * postgresql://dwhuser:***@dwhcluster.ckj67osxtkpg.us-east-1.redshift.amazonaws.com:5439/dev\n",
      "1 rows affected.\n"
     ]
    }
   ],
   "source": [
    "cnt_stage_stocks = %sql select count(*) from stocks.stage_stocks where \"open\" != 'null';\n",
    "cnt_fact_stocks = %sql select count(*) from stocks.fact_stocks;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact Load - Stocks: PASS\n"
     ]
    }
   ],
   "source": [
    "# Fact Stocks\n",
    "if cnt_fact_stocks[0][0] == cnt_stage_stocks[0][0]:\n",
    "    print('Fact Load - Stocks: PASS')\n",
    "else:\n",
    "    print('Fact Load - Stocks: FAIL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### CHECK #2: Validate no NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.ckj67osxtkpg.us-east-1.redshift.amazonaws.com:5439/dev\n",
      "1 rows affected.\n"
     ]
    }
   ],
   "source": [
    "cnt_null_fact_stocks = %sql select count(*) from stocks.fact_stocks where \"open\" is null or \"high\" is null or \"low\" is null or \"close\" is null or volume is null;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact Load - Stocks: FAIL\n"
     ]
    }
   ],
   "source": [
    "# Fact Stocks\n",
    "if cnt_null_fact_stocks[0][0] == '0':\n",
    "    print('Fact Load - Stocks: PASS')\n",
    "else:\n",
    "    print('Fact Load - Stocks: FAIL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "1. Fact Table: Stocks\n",
    "    - Column #1: Symbol - Stock/Ticker Symbol [Data Type: String]\n",
    "    - Column #2: Date - Price Date [Data Type: Date]\n",
    "    - Column #3: Open - Open Price [Data Type: Double]\n",
    "    - Column #4: Close - Close Price [Data Type: Double]\n",
    "    - Column #5: High - Intra-day high Price [Data Type: Double]\n",
    "    - Column #6: Low - Intra-day low Price [Data Type: Double]\n",
    "    - Column #7: Adjusted Close - Adjusted Close Price [Data Type: Double]\n",
    "    - Column #8: Volume - Traded Volume [Data Type: Integer]\n",
    "    \n",
    "    \n",
    "2. Dimension Table: Companies\n",
    "    - Column #1: Index - Running Number/Sequence [Data Type: Integer]\n",
    "    - Column #2: Symbol - Stock/Ticker Symbol [Data Type: String]\n",
    "    - Column #3: Company - Company Name [Data Type: String]\n",
    "\n",
    "\n",
    "3. Dimension Table: Date\n",
    "    - Column #1: Date - Date [Data Type: Date]\n",
    "    - Column #2: Year - Year extracted from Date [Data Type: Integer]\n",
    "    - Column #3: Quarter - Quarter extracted from Date [Data Type: Integer]\n",
    "    - Column #4: Month - Month extracted from Date [Data Type: Integer]\n",
    "    - Column #5: Week - Week extracted from Date [Data Type: Integer]\n",
    "    - Column #6: Day - Day extracted from Date [Data Type: Integer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "   * AWS S3 was choosen for staging source/intermediate files as it an elastic cloud storage and integrates well with Apache Spark and AWS Redshift. \n",
    "   * Apache Spark was choosen as the tool for data exploration as we are dealing with a huge number of files. Spark has the ability to process multiple files contained within a directory and there is no need for building looping logic (as would have been required with Pandas). \n",
    "   * AWS Redshift was choosen as the data warehouse as again it is cloud based, scalable and can host dimensional data models for downstrea analytics. \n",
    "\n",
    "\n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "   * As we are dealing with stock data, the data could be sourced daily, weekly or even monthly. Daily would be ideal but there would be additional maintaince/support over head due to increased number of jobs. The downstream usage of the data would also be a major factor here. If the analytics is only done on a monthly basis, the load can be monthly. However, if there is a need for more frequent / more current data, then the load frequency could be increased. \n",
    "\n",
    "\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "   * Same approach, as the volume for a day currently is < 1500 records. So, even if there is a 100x increase, the number is still managable. Even if the load process is monthly, the existing ETL would be able to scale up. \n",
    "   \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "    * Directory structure on S3 should be modified to support daily files. Additional Apache Airflow could be considered for workflow management inorder to schedule the daily loads.  \n",
    "    \n",
    " * The database needed to be accessed by 100+ people.\n",
    "    * Same approach. AWS Redshift can scale up to accomodate this large user base. However, based on the usage/queries executed, there may be a need to re-distribute the data or create replicas to cater to varies usage. Additionally, if a number of users aggregate and consume data of similar kind, there may be a case to introduce and populate aggregate tables to optimize the usage of the system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
